大家好 这个视频中 我想对基本的机器学习方法和思想做一个简要的回顾。 我想带大家回顾几个有名的机器学习算法。 线性模型， 基于树的方法，k近邻和神经网络。 对于这些类型的算法， 我会用样例给出一个简短的直观解释。 如果你不记得任何一个主题， 我强烈建议你通过额外材料给出的链接学习它。 让我们开始讨论线性模型。 假设我们有两个点集， 灰色的点属于一类，绿色的属于另一类。 直接用一条线划分出两类。 在这个例子中，因为是二维点集，很容易做到。 但这个方法可以扩展到高维空间。 这是线性模型的主要理念。 尝试通过超平面将空间划分为两部分，以此区分不同类型。 你可能记得这类模型的几个例子，比如逻辑回归或支持向量机（SVM）。 它们是拥有不同损失函数的线性模型。 我想强调的是线性模型 特别适用于高维稀疏数据。 但你应该记住线性模型的局限。 通常，点无法通过这样简单的方法划分。 比如，假设两个点集为环形， 一个点集在另一个内部。 虽然，怎么划分两类是显而易见的， 但线性模型并不是处理这个问题的恰当的选择，它不能划分开两类。 你可以在几乎所有的机器学习库中找到线性模型的实现。 大家都知道，Scikit-Learn库中实现了线性模型。 另外，在Vowpal Wabbit中的实现值得引起我们的注意， 因为它的设计是为了处理非常大的数据集。 我们已经回顾了线性模型，接下来回忆，基于树的方法簇。 基于树的方法将决策树 作为基础块，用于构建更复杂的模型。 让我们通过一个例子了解决策树的工作过程。 假设有两个近似于线性的点集， 通过平行于轴的线将两类分隔开。 我们使用一些限制条件来显著的减少可能的线的数目， 并且让我们用简单的方式来描述这些线。 划分之后如图所示， 将形成两个子空间， 上面的是灰色的概率是1， 下面的为灰色的概率为0.2。 上部分的子空间不需要进一步的划分。 现在，继续对下面的子空间进行划分。 现在，左边的子空间为灰色的概率是0，右边的概率是1。 这是决策树工作原理的简要回顾， 使用分治方法对子空间进行划分子空间。 直观地，单个决策树可以当做把空间划分进 盒子，并且使用盒中数据估计数据。 选择轴和相关常数的方式 造成了多个构建决策树的方法。 而且，树可以用多种方式组合在一起。 以上这些构成了广泛的基于树的算法， 其中最有名的是随机森林（random forest）和梯度提升决策树（GBDT）。 如果你不知道这些是什么， 我强烈建议你使用额外材料给出的链接去记住这些主题。 通常，树模型是非常强大的， 并且对于表格式数据，可以是一个很好的默认方法。 在几乎所有的竞赛中， 胜利者都使用这个方法。 但记住，对于基于树的方法 很难捕捉线性依赖，因为它需要很多次划分。 假设有两个可以线性划分的点集， 为了分割点集，生成树需要很多次划分。 如图所示，即使在这种情况下，树 在决策边界附近也可能不准确。 与线性模型相似， 你可以在几乎所有的机器学习库中 发现树模型的实现。 Scikit-Learn包含我个人认为非常好的 随机森林（RF）的实现。 Scikit-Learn包含所有的梯度提升决策树的实现。 为了更高的速度和准确率（accuracy）我更倾向使用XGBoost和LightGBM库。 因此，结束了基于树的方法，接下来是kNN（k近邻）。 在我开始讲解之前， 我想说，kNN是k近邻的缩写， 不要将它与神经网络混淆。 让我们看熟悉的二分类问题。 如，我们需要预测这张 幻灯片中带问号的点的标签。 假设彼此接近的点可能有相同的标签。 因此，我们需要找到箭头所指的 最近点，并且将它的标签作为预测结果。 这是最近邻方法的一般工作过程。 它很容易推广为k近邻， 若我们找到k个最近的点，通过多数表决选择加上标签。 k近邻直观上很简单， 更近的对象可能有相同的标签。 的分类问题 我们使用平方距离寻找最近点。 一般情况下，使用这种距离函数可能毫无意义。 例如，对于图像而言，平方距离不能捕获语义。 尽管方法简单， 最近邻算法中的特征通常富含信息量。 我们将在后续课程中讨论细节。 k近邻的实现可以在大多数机器学习库中找到。 我建议你使用Scikit-Learn实现的算法，因为它 使用矩阵算法加速记忆，并且 允许你使用几个预定义距离函数。 它也允许你实现自定义距离函数。 下一个回顾的大模型类，是神经网络。 神经网络是一类特殊的机器学习模型， 它应该作为一个独立的主题。 通常，如你所见，与决策树不同，在这个黑盒中的方法 生成了一个平滑的划分曲线。 我推荐你浏览幻灯片所示的TensorFlow场景， 并且对简单的前向回馈网络使用不同的参数， 从而对前向回馈网络的工作流程有一些直观的感受。 有些类别的神经网络特别适用于图像、 声音、文本和序列。 我们不会在这门课上谈论神经网络的细节。 自从近些年，神经网络吸引了大量的注意力， 因而有许多适用于神经网络的框架。 如TensorFlow、Keras、 MXNet、PyTorch和Lasagne可以用于训练神经网络。 我个人青睐于PyTorch，因为它提供 自由和用户友好的方式去定义复杂网络。 简单回顾之后， 我想简要谈论一下‘没有免费的午餐’定理（NFL）。 从根本上说，没有免费的午餐定理阐述了 没有方法在所有任务中都优于其他的方法， 换句话说，对于每一个方法， 我们都可以构造出一个任务，让它不是解决这个任务最好的方法。 因为每一个方法都依赖于数据或者任务的一些假设。 如果不满足假设， 被限制的方法将表现不佳。 对我们而言，这意味着我们不能使用一个算法参加每一个比赛。 所以，我们需要基于不同的假设使用各种工具。 在本节视频最后， 我想向你展示Scikit-Learn库的一个例子， 它绘制了不同分类器的决策面。 可以看出算法的种类对于决策边界和 答案的连续性有显著的影响。 我强烈推荐你深入到这个例子中，确保你 对于为什么这些分类器产生这样的表面有直观感觉。 最后，我想提醒你本视频的关键点。 首先，没有哪个算法在任何一个任务中 优于其他算法的。 其次，线性模型可以当做是通过超平面 将空间划分为两个子空间。 基于树的方法将空间划分进盒子中，并使用每个盒子的常数进行预测。 k近邻算法基于 相近对象可能有相同标签的假设。 所以，我们需要找到最近的对象，记录它们的标签。 k近邻方法也高度依赖于如何衡量点是否相近。 前向回馈神经网络更难解释，但 它们产生平滑的非线性决策边界。 最强大的方法是梯度提升决策树和神经网络。 但我们不应该低估线性模型 和k近邻，因为有时，它们可能会更好。 我们将在后续的课程中叙述相关的例子。谢谢你的关注。