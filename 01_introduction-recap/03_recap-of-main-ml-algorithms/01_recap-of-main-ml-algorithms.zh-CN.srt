1
00:00:03,150 --> 00:00:06,265
大家好 这个视频中

2
00:00:06,265 --> 00:00:11,460
我想对基本的机器学习方法和思想做一个简要的回顾。

3
00:00:11,460 --> 00:00:15,480
我想带大家回顾几个有名的机器学习算法。

4
00:00:15,480 --> 00:00:16,765
线性模型，

5
00:00:16,765 --> 00:00:20,220
基于树的方法，k近邻和神经网络。

6
00:00:20,220 --> 00:00:21,675
对于这些类型的算法，

7
00:00:21,675 --> 00:00:25,295
我会用样例给出一个简短的直观解释。

8
00:00:25,295 --> 00:00:27,660
如果你不记得任何一个主题，

9
00:00:27,660 --> 00:00:32,630
我强烈建议你通过额外材料给出的链接学习它。

10
00:00:32,630 --> 00:00:34,910
让我们开始讨论线性模型。

11
00:00:34,910 --> 00:00:37,648
假设我们有两个点集，

12
00:00:37,648 --> 00:00:41,640
灰色的点属于一类，绿色的属于另一类。

13
00:00:41,640 --> 00:00:45,405
直接用一条线划分出两类。

14
00:00:45,405 --> 00:00:49,810
在这个例子中，因为是二维点集，很容易做到。

15
00:00:49,810 --> 00:00:53,620
但这个方法可以扩展到高维空间。

16
00:00:53,620 --> 00:00:56,410
这是线性模型的主要理念。

17
00:00:56,410 --> 00:01:01,340
尝试通过超平面将空间划分为两部分，以此区分不同类型。

18
00:01:01,340 --> 00:01:07,700
你可能记得这类模型的几个例子，比如逻辑回归或支持向量机（SVM）。

19
00:01:07,700 --> 00:01:11,535
它们是拥有不同损失函数的线性模型。

20
00:01:11,535 --> 00:01:13,700
我想强调的是线性模型

21
00:01:13,700 --> 00:01:17,005
特别适用于高维稀疏数据。

22
00:01:17,005 --> 00:01:20,295
但你应该记住线性模型的局限。

23
00:01:20,295 --> 00:01:24,175
通常，点无法通过这样简单的方法划分。

24
00:01:24,175 --> 00:01:28,315
比如，假设两个点集为环形，

25
00:01:28,315 --> 00:01:30,085
一个点集在另一个内部。

26
00:01:30,085 --> 00:01:33,005
虽然，怎么划分两类是显而易见的，

27
00:01:33,005 --> 00:01:37,490
但线性模型并不是处理这个问题的恰当的选择，它不能划分开两类。

28
00:01:37,490 --> 00:01:42,545
你可以在几乎所有的机器学习库中找到线性模型的实现。

29
00:01:42,545 --> 00:01:45,570
大家都知道，Scikit-Learn库中实现了线性模型。

30
00:01:45,570 --> 00:01:49,120
另外，在Vowpal Wabbit中的实现值得引起我们的注意，

31
00:01:49,120 --> 00:01:52,585
因为它的设计是为了处理非常大的数据集。

32
00:01:52,585 --> 00:01:57,485
我们已经回顾了线性模型，接下来回忆，基于树的方法簇。

33
00:01:57,485 --> 00:02:00,210
基于树的方法将决策树

34
00:02:00,210 --> 00:02:03,175
作为基础块，用于构建更复杂的模型。

35
00:02:03,175 --> 00:02:06,185
让我们通过一个例子了解决策树的工作过程。

36
00:02:06,185 --> 00:02:10,305
假设有两个近似于线性的点集，

37
00:02:10,305 --> 00:02:15,995
通过平行于轴的线将两类分隔开。

38
00:02:15,995 --> 00:02:19,290
我们使用一些限制条件来显著的减少可能的线的数目，

39
00:02:19,290 --> 00:02:23,630
并且让我们用简单的方式来描述这些线。

40
00:02:23,630 --> 00:02:26,253
划分之后如图所示，

41
00:02:26,253 --> 00:02:27,985
将形成两个子空间，

42
00:02:27,985 --> 00:02:30,665
上面的是灰色的概率是1，

43
00:02:30,665 --> 00:02:34,005
下面的为灰色的概率为0.2。

44
00:02:34,005 --> 00:02:37,330
上部分的子空间不需要进一步的划分。

45
00:02:37,330 --> 00:02:40,300
现在，继续对下面的子空间进行划分。

46
00:02:40,300 --> 00:02:45,750
现在，左边的子空间为灰色的概率是0，右边的概率是1。

47
00:02:45,750 --> 00:02:49,220
这是决策树工作原理的简要回顾，

48
00:02:49,220 --> 00:02:53,950
使用分治方法对子空间进行划分子空间。

49
00:02:53,950 --> 00:02:57,770
直观地，单个决策树可以当做把空间划分进

50
00:02:57,770 --> 00:03:02,500
盒子，并且使用盒中数据估计数据。

51
00:03:02,500 --> 00:03:05,410
选择轴和相关常数的方式

52
00:03:05,410 --> 00:03:08,525
造成了多个构建决策树的方法。

53
00:03:08,525 --> 00:03:12,695
而且，树可以用多种方式组合在一起。

54
00:03:12,695 --> 00:03:16,338
以上这些构成了广泛的基于树的算法，

55
00:03:16,338 --> 00:03:21,005
其中最有名的是随机森林（random forest）和梯度提升决策树（GBDT）。

56
00:03:21,005 --> 00:03:23,075
如果你不知道这些是什么，

57
00:03:23,075 --> 00:03:27,675
我强烈建议你使用额外材料给出的链接去记住这些主题。

58
00:03:27,675 --> 00:03:30,515
通常，树模型是非常强大的，

59
00:03:30,515 --> 00:03:33,320
并且对于表格式数据，可以是一个很好的默认方法。

60
00:03:33,320 --> 00:03:35,055
在几乎所有的竞赛中，

61
00:03:35,055 --> 00:03:37,315
胜利者都使用这个方法。

62
00:03:37,315 --> 00:03:39,290
但记住，对于基于树的方法

63
00:03:39,290 --> 00:03:44,870
很难捕捉线性依赖，因为它需要很多次划分。

64
00:03:44,870 --> 00:03:49,535
假设有两个可以线性划分的点集，

65
00:03:49,535 --> 00:03:54,765
为了分割点集，生成树需要很多次划分。

66
00:03:54,765 --> 00:03:58,965
如图所示，即使在这种情况下，树

67
00:03:58,965 --> 00:04:00,640
在决策边界附近也可能不准确。

68
00:04:00,640 --> 00:04:02,731
与线性模型相似，

69
00:04:02,731 --> 00:04:04,030
你可以在几乎所有的机器学习库中

70
00:04:04,030 --> 00:04:07,225
发现树模型的实现。

71
00:04:07,225 --> 00:04:10,010
Scikit-Learn包含我个人认为非常好的

72
00:04:10,010 --> 00:04:12,940
随机森林（RF）的实现。

73
00:04:12,940 --> 00:04:17,385
Scikit-Learn包含所有的梯度提升决策树的实现。

74
00:04:17,385 --> 00:04:23,260
为了更高的速度和准确率（accuracy）我更倾向使用XGBoost和LightGBM库。

75
00:04:23,260 --> 00:04:28,775
因此，结束了基于树的方法，接下来是kNN（k近邻）。

76
00:04:28,775 --> 00:04:30,940
在我开始讲解之前，

77
00:04:30,940 --> 00:04:35,040
我想说，kNN是k近邻的缩写，

78
00:04:35,040 --> 00:04:37,815
不要将它与神经网络混淆。

79
00:04:37,815 --> 00:04:41,790
让我们看熟悉的二分类问题。

80
00:04:41,790 --> 00:04:44,250
如，我们需要预测这张

81
00:04:44,250 --> 00:04:47,005
幻灯片中带问号的点的标签。

82
00:04:47,005 --> 00:04:52,765
假设彼此接近的点可能有相同的标签。

83
00:04:52,765 --> 00:04:55,558
因此，我们需要找到箭头所指的

84
00:04:55,558 --> 00:04:59,740
最近点，并且将它的标签作为预测结果。

85
00:04:59,740 --> 00:05:03,195
这是最近邻方法的一般工作过程。

86
00:05:03,195 --> 00:05:05,350
它很容易推广为k近邻，

87
00:05:05,350 --> 00:05:10,685
若我们找到k个最近的点，通过多数表决选择加上标签。

88
00:05:10,685 --> 00:05:14,270
k近邻直观上很简单，

89
00:05:14,270 --> 00:05:17,250
更近的对象可能有相同的标签。

90
00:05:17,250 --> 00:05:19,165
的分类问题

91
00:05:19,165 --> 00:05:22,350
我们使用平方距离寻找最近点。

92
00:05:22,350 --> 00:05:26,175
一般情况下，使用这种距离函数可能毫无意义。

93
00:05:26,175 --> 00:05:31,630
例如，对于图像而言，平方距离不能捕获语义。

94
00:05:31,630 --> 00:05:34,070
尽管方法简单，

95
00:05:34,070 --> 00:05:37,520
最近邻算法中的特征通常富含信息量。

96
00:05:37,520 --> 00:05:41,065
我们将在后续课程中讨论细节。

97
00:05:41,065 --> 00:05:45,830
k近邻的实现可以在大多数机器学习库中找到。

98
00:05:45,830 --> 00:05:49,607
我建议你使用Scikit-Learn实现的算法，因为它

99
00:05:49,607 --> 00:05:52,240
使用矩阵算法加速记忆，并且

100
00:05:52,240 --> 00:05:55,740
允许你使用几个预定义距离函数。

101
00:05:55,740 --> 00:05:59,195
它也允许你实现自定义距离函数。

102
00:05:59,195 --> 00:06:03,520
下一个回顾的大模型类，是神经网络。

103
00:06:03,520 --> 00:06:07,115
神经网络是一类特殊的机器学习模型，

104
00:06:07,115 --> 00:06:09,310
它应该作为一个独立的主题。

105
00:06:09,310 --> 00:06:12,425
通常，如你所见，与决策树不同，在这个黑盒中的方法

106
00:06:12,425 --> 00:06:16,570
生成了一个平滑的划分曲线。

107
00:06:16,570 --> 00:06:20,390
我推荐你浏览幻灯片所示的TensorFlow场景，

108
00:06:20,390 --> 00:06:23,810
并且对简单的前向回馈网络使用不同的参数，

109
00:06:23,810 --> 00:06:28,085
从而对前向回馈网络的工作流程有一些直观的感受。

110
00:06:28,085 --> 00:06:32,242
有些类别的神经网络特别适用于图像、

111
00:06:32,242 --> 00:06:35,085
声音、文本和序列。

112
00:06:35,085 --> 00:06:38,385
我们不会在这门课上谈论神经网络的细节。

113
00:06:38,385 --> 00:06:42,400
自从近些年，神经网络吸引了大量的注意力，

114
00:06:42,400 --> 00:06:44,935
因而有许多适用于神经网络的框架。

115
00:06:44,935 --> 00:06:47,360
如TensorFlow、Keras、

116
00:06:47,360 --> 00:06:52,070
MXNet、PyTorch和Lasagne可以用于训练神经网络。

117
00:06:52,070 --> 00:06:54,817
我个人青睐于PyTorch，因为它提供

118
00:06:54,817 --> 00:06:58,190
自由和用户友好的方式去定义复杂网络。

119
00:06:58,190 --> 00:06:59,865
简单回顾之后，

120
00:06:59,865 --> 00:07:03,520
我想简要谈论一下‘没有免费的午餐’定理（NFL）。

121
00:07:03,520 --> 00:07:07,290
从根本上说，没有免费的午餐定理阐述了

122
00:07:07,290 --> 00:07:11,235
没有方法在所有任务中都优于其他的方法，

123
00:07:11,235 --> 00:07:14,040
换句话说，对于每一个方法，

124
00:07:14,040 --> 00:07:19,395
我们都可以构造出一个任务，让它不是解决这个任务最好的方法。

125
00:07:19,395 --> 00:07:24,505
因为每一个方法都依赖于数据或者任务的一些假设。

126
00:07:24,505 --> 00:07:26,640
如果不满足假设，

127
00:07:26,640 --> 00:07:28,470
被限制的方法将表现不佳。

128
00:07:28,470 --> 00:07:33,570
对我们而言，这意味着我们不能使用一个算法参加每一个比赛。

129
00:07:33,570 --> 00:07:37,705
所以，我们需要基于不同的假设使用各种工具。

130
00:07:37,705 --> 00:07:39,510
在本节视频最后，

131
00:07:39,510 --> 00:07:42,477
我想向你展示Scikit-Learn库的一个例子，

132
00:07:42,477 --> 00:07:46,115
它绘制了不同分类器的决策面。

133
00:07:46,115 --> 00:07:48,706
可以看出算法的种类对于决策边界和

134
00:07:48,706 --> 00:07:52,940
答案的连续性有显著的影响。

135
00:07:52,940 --> 00:07:57,170
我强烈推荐你深入到这个例子中，确保你

136
00:07:57,170 --> 00:08:01,635
对于为什么这些分类器产生这样的表面有直观感觉。

137
00:08:01,635 --> 00:08:05,700
最后，我想提醒你本视频的关键点。

138
00:08:05,700 --> 00:08:08,590
首先，没有哪个算法在任何一个任务中

139
00:08:08,590 --> 00:08:12,840
优于其他算法的。

140
00:08:12,840 --> 00:08:15,714
其次，线性模型可以当做是通过超平面

141
00:08:15,714 --> 00:08:19,485
将空间划分为两个子空间。

142
00:08:19,485 --> 00:08:26,145
基于树的方法将空间划分进盒子中，并使用每个盒子的常数进行预测。

143
00:08:26,145 --> 00:08:29,250
k近邻算法基于

144
00:08:29,250 --> 00:08:32,220
相近对象可能有相同标签的假设。

145
00:08:32,220 --> 00:08:35,685
所以，我们需要找到最近的对象，记录它们的标签。

146
00:08:35,685 --> 00:08:40,075
k近邻方法也高度依赖于如何衡量点是否相近。

147
00:08:40,075 --> 00:08:43,135
前向回馈神经网络更难解释，但

148
00:08:43,135 --> 00:08:46,580
它们产生平滑的非线性决策边界。

149
00:08:46,580 --> 00:08:51,630
最强大的方法是梯度提升决策树和神经网络。

150
00:08:51,630 --> 00:08:53,595
但我们不应该低估线性模型

151
00:08:53,595 --> 00:08:56,795
和k近邻，因为有时，它们可能会更好。

152
00:08:56,795 --> 00:09:02,350
我们将在后续的课程中叙述相关的例子。谢谢你的关注。