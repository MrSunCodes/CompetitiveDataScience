Hi, everyone. In this video, I will tell you about the specifics of Numerai Competition that was held throughout year 2016. Note that Numerai organizers changed the format in 2017. So, the findings I'm going to read will not work on new data. Let's state the problem. Participants were solving a binary classification task on a data set with 21 anonymized numeric features. Unusual part is that both train and test data sets have been updating every week. Data sets were also shuffled column-wise. So it was like a new task every week. Pretty challenging. As it turned out, this competition had a data leak. Organizers did not disclose any information about the nature of data set. But allegedly, it was some time series data with target variable highly dependent on transitions between time points. Think of something like predicting price change in stock market here. Means that, if we knew true order or had timestamp variable, we could easily get nearly perfect score. And therefore, we had to somehow reconstruct this order. Of course, approximately. But even a rough approximation was giving a huge advantage over other participants. The first and most important step is to find a nearest neighbor for every point in a data set, and add all 21 features from that neighbor to original point. Simple logistic regression of those 42 features, 21 from original, and 21 from neighboring points, allowed to get into top 10 on the leader board. Of course, we can get better scores with some Hardcore EDA. Let's start exploring correlation metrics of new 21 features. If group features with highest correlation coefficient next to each other, we'll get a right picture. This picture can help us in two different ways. First, we can actually fix some column order. So, weekly column shuffling won't affect our models. And second, we can clearly notice seven groups with three highly correlated features in each of them. So, the data actually has some non-trivial structure. Now, let's remember that we get new data sets every week. What is more? Each week, train data sets have the same number of points. We can assume that there is some connection between consecutive data sets. This is a little strange because we already have a time series. So, what's the connection between the data from different weeks? Well, if we find nearest neighbors from every point in current data set from previous data set, and plot distance distributions, we can notice that first neighbor is much, much closer than the second. So, we indeed have some connection between consecutive data sets. And it looks like we can build a bijective mapping between them. But let's not quickly jump into conclusions and do more exploration. Okay. We found a nearest neighbor in previous data set. What if we examine the distances between the neighboring objects at the level of individual features? We clearly have three different groups of seven features. Now remember, the sorted correlation matrix? It turns out that each of three highly correlated features belong to a different group. A perfect match. And if we multiply seven features from the first group by three, and seven features from the second group by two in the original data set, recalculate nearest neighbor-based features within the data sets, and re-train our models, we'll get a nice improvement. So, after this magic multiplications, of course, I'd tried other constants, our true order approximation became a little better. Great. Now, let's move to the true relation. New data, weekly updates, all of it was a lie. Remember, how we were calculating neighbors between consecutive data sets? Well, we can forget about consecutiveness. Calculate neighbors between current data set, and the data set from two weeks ago or two months ago. No matter what, we will be getting pretty much the same distances. Why? The simplest answer is that the data actually didn't change. And every week, we were getting the same data, plus a little bit of noise. And thus, we could find nearest neighbor in each of previous data sets, and average them all, successfully reducing the variance of added noise. After averaging, true order approximation became even better. I have to say that a little bit of test data actually did change from time to time. But nonetheless, most of the roles migrated from week to week. Because of that, it was possible to probe the whole public leader board which helped even further, and so on, and so on. Of course, there are more details regarding that competition, but they aren't very interesting. I wanted to focus on the process of reverse engineering. Anyway, I hope you like this kind of detective story and realized how important exploratory data analysis could be. Thank you for your attention and always pay respect to EDA.